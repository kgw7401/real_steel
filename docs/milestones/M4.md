# Milestone 4: Evaluation Pipeline

**Status:** Not Started
**Ref:** `docs/PROJECT.md`, `CLAUDE.md`

## Objective

Build an evaluation pipeline that measures how accurately the system reproduces human motion at each stage. After M4, running `python src/main.py --sim --record --eval` records every frame's data to JSONL, displays real-time accuracy metrics on-screen, and produces session reports. Stored data can be replayed offline to test parameter changes without a live camera.

The pipeline measures two gaps:

```
[Real Motion] ──Gap A── [MediaPipe Detection] ──Gap B── [Simulation Execution]
   (human)                  (perceived)                     (robot)
```

- **Gap A** (Detection accuracy): How well MediaPipe captures the real motion. Measured via static pose benchmarks and dynamic motion sequence evaluation.
- **Gap B** (Execution accuracy): How well the simulator tracks the commanded angles. Measured every frame as `|robot_cmd - robot_actual|`.

## Dependencies

- M0 complete (venv, dependencies)
- M1 complete (URDF, `SimulatedRobot`)
- M2 complete (full perception pipeline, `main.py`)
- Webcam available

## Out of Scope

- Hardware Gap (Simulation ↔ Real Hardware) — deferred until hardware arrives
- Learning-based motion mapping (future milestone)
- Custom pose model training
- Motion capture ground truth (Vicon, OptiTrack)

---

## Key Design Decisions

### 1. Gap A uses both static poses and dynamic motion sequences

Static poses (T-pose, guard) verify angle accuracy at known positions. Dynamic sequences (jab, cross, hook) verify trajectory, timing, and transitions. Boxing motions have well-defined "correct form" that serves as ground truth without motion capture equipment.

### 2. JSONL for session storage (start simple)

Every frame produces one JSON object (one line in a JSONL file). The schema captures raw MediaPipe output, computed angles, robot commands, robot actual state, and gap metrics. Nothing more. JSONL is human-readable, requires no extra dependencies, and is sufficient for initial development. See "Future: Storage Format Migration" for when to reconsider.

### 3. Offline replay enables agent-driven improvement

Stored landmark data can be re-processed with different parameters (smoothing factor, PD gains, etc.) without running the camera again. This is the foundation for automated improvement — an agent reads the session data, diagnoses the largest gap source, modifies code/config, and validates via offline replay.

---

## Data Schema

### Frame Record (one JSON object per line in JSONL)

```json
{
  // Metadata
  "timestamp": 1705312200.123,       // Unix timestamp
  "frame_number": 42,

  // Stage 1: MediaPipe raw output
  "landmarks": [[x, y, z, vis], ...],  // 33 landmarks, array of [x, y, z, visibility]
  "pose_confidence": 0.85,             // average visibility of 6 upper-body keypoints

  // Stage 2: Computed human angles
  "human_angles": [0.1, 0.2, ...],   // 8 joint angles (radians)

  // Stage 3: Robot command and actual state
  "robot_cmd": [0.1, 0.2, ...],      // 8 commanded angles (radians)
  "robot_actual": [0.1, 0.2, ...],   // 8 actual angles from simulator (radians)

  // Gap B (computed per frame)
  "gap_b": [0.01, 0.02, ...],        // |robot_cmd - robot_actual| per joint (radians)
  "gap_b_rmse": 0.015,               // RMSE across all joints (radians)

  // Performance
  "latency_ms": 14.2,                // total pipeline latency

  // Motion context (optional, set during guided test sequences)
  "motion_label": "jab",             // e.g. "jab", "cross", null for free motion
  "motion_phase": "extend"           // e.g. "guard", "extend", "retract", null
}
```

Using nested arrays for landmarks keeps the structure natural for JSON (no column flattening needed). numpy arrays are converted to lists on write.

### Session Storage

```
data/
├── sessions/
│   ├── 2024-01-15_14-30-00/
│   │   ├── frames.jsonl                # one JSON object per frame
│   │   └── metadata.json              # session summary + config snapshot
│   ├── 2024-01-15_15-00-00/
│   │   ├── frames.jsonl
│   │   └── metadata.json
│   └── ...
└── benchmarks/
    ├── benchmark_2024-01-15.json       # static + dynamic test results
    └── ...
```

### Session Metadata (`metadata.json` — separate file per session)

```json
{
  "session_id": "2024-01-15_14-30-00",
  "mode": "simulation",
  "duration_sec": 120.5,
  "total_frames": 5847,
  "config_snapshot": {
    "smoothing_factor": 0.3,
    "dead_zone_deg": 2.0,
    "mirror_mode": true,
    "pybullet_kp": 1.0,
    "pybullet_kd": 0.1
  },
  "avg_gap_b_rmse_deg": 5.2,
  "avg_latency_ms": 14.2,
  "avg_pose_confidence": 0.85
}
```

---

## Motion Sequence Definitions

### Static Poses (Gap A — pose accuracy)

```yaml
# config/test_poses.yaml

static_poses:
  t_pose:
    description: "Arms horizontal, fully extended"
    hold_seconds: 3
    expected_angles_deg:
      l_shoulder_roll: 90
      l_shoulder_tilt: 0
      l_shoulder_pan: 0
      l_elbow: 0
      r_shoulder_roll: 90
      r_shoulder_tilt: 0
      r_shoulder_pan: 0
      r_elbow: 0
    tolerance_deg: 10

  arms_down:
    description: "Arms at sides (attention pose)"
    hold_seconds: 3
    expected_angles_deg:
      l_shoulder_roll: 0
      l_shoulder_tilt: 0
      l_shoulder_pan: 0
      l_elbow: 0
      r_shoulder_roll: 0
      r_shoulder_tilt: 0
      r_shoulder_pan: 0
      r_elbow: 0
    tolerance_deg: 10

  guard:
    description: "Boxing guard position"
    hold_seconds: 3
    expected_angles_deg:
      l_shoulder_roll: 30
      l_shoulder_tilt: 20
      l_shoulder_pan: 10
      l_elbow: 90
      r_shoulder_roll: 30
      r_shoulder_tilt: 20
      r_shoulder_pan: 10
      r_elbow: 90
    tolerance_deg: 12

  elbow_90:
    description: "Both elbows bent at 90 degrees"
    hold_seconds: 3
    expected_angles_deg:
      l_elbow: 90
      r_elbow: 90
    tolerance_deg: 8
```

### Dynamic Motion Sequences (Gap A — trajectory accuracy)

```yaml
# config/test_sequences.yaml

motion_sequences:
  left_jab:
    description: "Left jab - quick straight punch"
    keyframes:
      - phase: "guard"
        time_ratio: 0.0
        angles_deg:
          l_elbow: 90
          l_shoulder_tilt: 20
        tolerance_deg: 12

      - phase: "extend"
        time_ratio: 0.4
        angles_deg:
          l_elbow: 0
          l_shoulder_tilt: 45
        tolerance_deg: 10

      - phase: "retract"
        time_ratio: 1.0
        angles_deg:
          l_elbow: 90
          l_shoulder_tilt: 20
        tolerance_deg: 12

    trajectory_constraints:
      - joint: "l_elbow"
        pattern: "decrease_then_increase"    # extends then retracts
      - joint: "l_shoulder_tilt"
        pattern: "increase_then_decrease"    # raises then lowers

    timing:
      duration_range_sec: [0.2, 0.8]         # jab is fast

  right_cross:
    description: "Right cross - power punch across body"
    keyframes:
      - phase: "guard"
        time_ratio: 0.0
        angles_deg:
          r_elbow: 90
          r_shoulder_tilt: 20
          r_shoulder_pan: 10
        tolerance_deg: 12

      - phase: "extend"
        time_ratio: 0.5
        angles_deg:
          r_elbow: 0
          r_shoulder_tilt: 45
          r_shoulder_pan: -30
        tolerance_deg: 10

      - phase: "retract"
        time_ratio: 1.0
        angles_deg:
          r_elbow: 90
          r_shoulder_tilt: 20
          r_shoulder_pan: 10
        tolerance_deg: 12

    trajectory_constraints:
      - joint: "r_elbow"
        pattern: "decrease_then_increase"
      - joint: "r_shoulder_pan"
        pattern: "decrease_then_increase"    # crosses midline then returns

    timing:
      duration_range_sec: [0.3, 1.0]

  left_hook:
    description: "Left hook - horizontal arc punch"
    keyframes:
      - phase: "guard"
        time_ratio: 0.0
        angles_deg:
          l_elbow: 90
          l_shoulder_pan: 10
        tolerance_deg: 12

      - phase: "swing"
        time_ratio: 0.5
        angles_deg:
          l_elbow: 90           # elbow stays bent
          l_shoulder_pan: -60   # arm sweeps horizontally
        tolerance_deg: 15

      - phase: "retract"
        time_ratio: 1.0
        angles_deg:
          l_elbow: 90
          l_shoulder_pan: 10
        tolerance_deg: 12

    trajectory_constraints:
      - joint: "l_elbow"
        pattern: "stable"                    # stays near 90
        stable_range_deg: 20
      - joint: "l_shoulder_pan"
        pattern: "decrease_then_increase"    # sweeps out then back

    timing:
      duration_range_sec: [0.3, 1.0]

  right_uppercut:
    description: "Right uppercut - upward punch"
    keyframes:
      - phase: "guard"
        time_ratio: 0.0
        angles_deg:
          r_elbow: 90
          r_shoulder_tilt: 20
        tolerance_deg: 12

      - phase: "drop"
        time_ratio: 0.3
        angles_deg:
          r_elbow: 100
          r_shoulder_tilt: 0
        tolerance_deg: 15

      - phase: "rise"
        time_ratio: 0.6
        angles_deg:
          r_elbow: 70
          r_shoulder_tilt: 60
        tolerance_deg: 12

      - phase: "retract"
        time_ratio: 1.0
        angles_deg:
          r_elbow: 90
          r_shoulder_tilt: 20
        tolerance_deg: 12

    trajectory_constraints:
      - joint: "r_shoulder_tilt"
        pattern: "decrease_then_increase"    # dips then rises

    timing:
      duration_range_sec: [0.4, 1.2]
```

---

## Deliverables

| # | Artifact | Path | Description |
|---|----------|------|-------------|
| D1 | Recorder | `src/evaluation/recorder.py` | Collects frame data and writes to JSONL |
| D2 | Evaluator | `src/evaluation/evaluator.py` | Computes Gap A/B metrics, static + dynamic |
| D3 | Visualizer | `src/evaluation/visualizer.py` | Real-time accuracy overlay on camera feed |
| D4 | Replay | `src/evaluation/replay.py` | Offline re-processing with different parameters |
| D5 | Test poses config | `config/test_poses.yaml` | Static pose definitions |
| D6 | Test sequences config | `config/test_sequences.yaml` | Dynamic motion sequence definitions |
| D7 | Main.py update | `src/main.py` | `--record`, `--eval`, `--run-test` flags |
| D8 | Eval tests | `tests/test_evaluation.py` | Unit tests for evaluation pipeline |

---

## Technical Spec

### S1: `src/evaluation/recorder.py`

Records pipeline data to JSONL files (one JSON object per line).

```python
class Recorder:
    def __init__(self, output_dir: str = "data/sessions")
    def start_session(self, config_snapshot: dict) -> str
    def record_frame(self,
        timestamp: float,
        frame_number: int,
        landmarks: np.ndarray | None,    # (33, 4) or None if no detection
        pose_confidence: float,
        human_angles: np.ndarray,        # (8,)
        robot_cmd: np.ndarray,           # (8,)
        robot_actual: np.ndarray,        # (8,)
        latency_ms: float,
        motion_label: str | None = None,
        motion_phase: str | None = None,
    ) -> None
    def end_session(self) -> Path
```

**`start_session(config_snapshot)`:**
1. Generate session ID from current datetime: `YYYY-MM-DD_HH-MM-SS`
2. Create session directory: `{output_dir}/{session_id}/`
3. Open `frames.jsonl` file handle for streaming writes
4. Store config snapshot for metadata file
5. Return session ID

**`record_frame(...)`:**
1. Compute `gap_b = np.abs(robot_cmd - robot_actual)` per joint
2. Compute `gap_b_rmse = np.sqrt(np.mean(gap_b ** 2))`
3. Convert numpy arrays to lists for JSON serialization
4. Build frame dict with all fields
5. Write one JSON line to the open file handle (`json.dumps(frame) + "\n"`)

**`end_session()`:**
1. Close the JSONL file handle
2. Compute session summary stats by re-reading the JSONL file (or from running accumulators)
3. Write `metadata.json` with session summary + config snapshot
4. Return session directory path

**Write strategy:**
- Stream frames directly to disk via an open file handle (no buffering in memory)
- Each `record_frame()` call writes one line immediately
- No memory accumulation — session length is limited only by disk space
- `metadata.json` is written once at `end_session()`

### S2: `src/evaluation/evaluator.py`

Computes Gap A and Gap B metrics.

```python
@dataclass
class GapMetrics:
    per_joint_deg: np.ndarray       # (8,) mean error per joint in degrees
    rmse_deg: float                 # overall RMSE in degrees
    max_error_deg: float            # worst single-joint error
    worst_joint_idx: int            # index of worst joint
    accuracy_percent: float         # 100 - (rmse / tolerance) * 100, clamped [0, 100]

@dataclass
class MotionEvalResult:
    motion_name: str
    keyframe_errors: dict           # {phase_name: GapMetrics}
    trajectory_pass: dict           # {constraint_desc: bool}
    timing_pass: bool
    overall_score: float            # 0-100

class Evaluator:
    def __init__(self, tolerance_deg: float = 15.0)

    # Gap B: per-frame (called every frame in real-time)
    def compute_gap_b(self, robot_cmd: np.ndarray, robot_actual: np.ndarray) -> GapMetrics

    # Gap A static: compare against known pose
    def evaluate_static_pose(self,
        human_angles: np.ndarray,
        expected_angles_deg: dict,
        tolerance_deg: float
    ) -> GapMetrics

    # Gap A dynamic: evaluate a recorded motion sequence
    def evaluate_motion_sequence(self,
        recorded_angles: list[np.ndarray],  # time series of human_angles
        timestamps: list[float],
        motion_def: dict                     # from test_sequences.yaml
    ) -> MotionEvalResult

    # Running average (for real-time display)
    def update_running_stats(self, gap_b: GapMetrics) -> GapMetrics
    def get_running_stats(self) -> GapMetrics
```

**`compute_gap_b(robot_cmd, robot_actual)`:**
1. `error = np.abs(robot_cmd - robot_actual)`
2. `error_deg = np.rad2deg(error)`
3. Return `GapMetrics` with per-joint, RMSE, max, worst joint

**`evaluate_static_pose(human_angles, expected_angles_deg, tolerance_deg)`:**
1. Convert expected angles from degrees to radians
2. Compute error only for joints specified in `expected_angles_deg` (some poses only check certain joints)
3. Return `GapMetrics`

**`evaluate_motion_sequence(recorded_angles, timestamps, motion_def)`:**
1. Normalize timestamps to [0.0, 1.0] range
2. For each keyframe in motion_def:
   - Find the frame closest to `keyframe.time_ratio`
   - Compute angle error at that frame
3. For each trajectory constraint:
   - Extract the relevant joint's angle time series
   - Check pattern ("decrease_then_increase", "stable", etc.)
   - "decrease_then_increase": verify there's a minimum point, values decrease before and increase after
   - "stable": verify values stay within `stable_range_deg` of mean
4. Check timing: total duration within `duration_range_sec`
5. Compute overall score: weighted average of keyframe accuracy, trajectory pass rate, timing

### S3: `src/evaluation/visualizer.py`

Draws real-time accuracy info on the camera frame.

```python
class EvalVisualizer:
    def __init__(self)

    def draw_gap_overlay(self, frame: np.ndarray, gap_b: GapMetrics) -> np.ndarray
    def draw_test_guide(self, frame: np.ndarray, instruction: str, countdown: float) -> np.ndarray
```

**`draw_gap_overlay(frame, gap_b)`:**

Draws on the right side of the camera frame:

```
┌─────────────────────────────────┐
│                   GAP B: 4.2°   │
│                   ════════════  │
│                                 │
│   Camera Feed     L_roll  2.1°  │
│   + Skeleton      L_tilt  3.4°  │
│                   L_pan   8.2°  │
│                   L_elbow 1.0°  │
│                   R_roll  2.3°  │
│                   R_tilt  3.1°  │
│                   R_pan   7.9°  │
│                   R_elbow 0.8°  │
│                                 │
│                   ACC: 87%      │
└─────────────────────────────────┘
```

- Per-joint bars: green (<5°), yellow (5-10°), red (>10°)
- Overall accuracy percentage

**`draw_test_guide(frame, instruction, countdown)`:**

Shows test instructions during guided test sequences:

```
┌─────────────────────────────────┐
│  ┌───────────────────────────┐  │
│  │  HOLD: T-POSE    2.1s    │  │
│  └───────────────────────────┘  │
│                                 │
│          Camera Feed            │
│                                 │
└─────────────────────────────────┘
```

### S4: `src/evaluation/replay.py`

Replays stored session data with different parameters.

```python
class Replay:
    def __init__(self)

    def load_session(self, session_dir: str) -> list[dict]
    def replay_with_config(self,
        frames: list[dict],
        new_config: dict
    ) -> list[dict]
    def compare(self,
        original_frames: list[dict],
        replayed_frames: list[dict]
    ) -> dict
```

**`load_session(session_dir)`:**
1. Read `{session_dir}/frames.jsonl` line by line
2. Parse each line as JSON
3. Return list of frame dicts

**`replay_with_config(frames, new_config)`:**
1. Extract stored landmarks from each frame dict
2. Re-instantiate `AngleCalculator` with `new_config["smoothing_factor"]`
3. Re-instantiate `MotionMapper` with new config values
4. For each frame:
   - Reconstruct PoseResult from stored landmarks
   - Recalculate `human_angles` with new AngleCalculator
   - Recalculate `robot_cmd` with new MotionMapper
   - (Skip robot_actual — would need PyBullet re-simulation)
5. Return new list of frame dicts with recalculated values

**`compare(original_frames, replayed_frames)`:**
1. Compute per-joint gap improvement: `original_gap - replayed_gap`
2. Return summary:
```python
{
    "original_avg_gap_deg": 5.2,
    "new_avg_gap_deg": 3.8,
    "improvement_deg": 1.4,
    "improvement_percent": 26.9,
    "per_joint_improvement": {...}
}
```

**Limitation:** Replay recalculates angles and commands but cannot re-simulate PyBullet execution. Gap B from replay reflects only angle calculation and mapping changes, not PD gain changes. To test PD gain changes, a separate PyBullet replay mode would be needed (future enhancement).

### S5: `src/main.py` Update

Add three new CLI flags:

```
--record     Enable data recording (saves to data/sessions/)
--eval       Enable real-time Gap B display overlay
--run-test   Run guided test sequence: "static", "dynamic", or "all"
```

**Integration in main loop:**

```python
# Setup
if args.record:
    recorder = Recorder()
    session_id = recorder.start_session(config_snapshot=cfg)

if args.eval:
    evaluator = Evaluator(tolerance_deg=cfg.get("evaluation", {}).get("tolerance_deg", 15.0))
    eval_viz = EvalVisualizer()

# Per-frame (inside main loop)
if args.record:
    recorder.record_frame(
        timestamp=frame.timestamp,
        frame_number=frame.frame_number,
        landmarks=pose.raw_landmarks,
        pose_confidence=pose.avg_confidence,
        human_angles=human_angles,
        robot_cmd=robot_cmd,
        robot_actual=robot.get_joint_state().positions,
        latency_ms=profiler.total_ms,
    )

if args.eval:
    gap_b = evaluator.compute_gap_b(robot_cmd, robot_actual)
    evaluator.update_running_stats(gap_b)
    frame_image = eval_viz.draw_gap_overlay(frame_image, evaluator.get_running_stats())

# Cleanup
if args.record:
    path = recorder.end_session()
    print(f"Session saved to {path}")
```

**`--run-test` mode:**

```python
if args.run_test:
    test_runner = TestRunner(evaluator, recorder, eval_viz)

    if args.run_test in ("static", "all"):
        test_runner.run_static_poses("config/test_poses.yaml")

    if args.run_test in ("dynamic", "all"):
        test_runner.run_motion_sequences("config/test_sequences.yaml")

    test_runner.save_report("data/benchmarks/")
```

The `TestRunner` guides the user through each pose/motion:
1. Display instruction on screen (e.g., "Hold T-POSE")
2. Countdown timer
3. Record frames during hold/motion period
4. Compute Gap A for each pose/motion
5. Show pass/fail result
6. Continue to next test

### S6: Replay CLI Tool

```bash
# Compare current config vs modified config
python -m src.evaluation.replay \
    --session data/sessions/2024-01-15_14-30-00 \
    --config config/new_settings.yaml

# Output:
# Original avg Gap B: 5.2°
# New avg Gap B:      3.8°
# Improvement:        1.4° (26.9%)
#
# Per-joint changes:
#   l_shoulder_pan:  9.2° → 5.1° (-44.6%)
#   l_shoulder_tilt: 4.8° → 4.5° (-6.3%)
#   ...
```

---

## Tasks

### Task 4.1: Create evaluation package structure

Create the `src/evaluation/` package with `__init__.py`.

**Produces:** `src/evaluation/__init__.py`
**Done when:** `from src.evaluation import ...` succeeds

### Task 4.2: Define data schema and implement Recorder

Implement `src/evaluation/recorder.py` per S1. Create the `data/sessions/` directory.

**Requires:** Task 4.1
**Produces:** `src/evaluation/recorder.py`
**Done when:** Unit test records 100 frames and writes a valid JSONL file + metadata.json

### Task 4.3: Implement Evaluator (Gap A + Gap B)

Implement `src/evaluation/evaluator.py` per S2. Support both static pose evaluation and dynamic motion sequence evaluation.

**Requires:** Task 4.1
**Produces:** `src/evaluation/evaluator.py`
**Done when:** Unit tests pass for Gap B computation, static pose evaluation, and motion sequence evaluation with known inputs

### Task 4.4: Create test pose and motion sequence configs

Write `config/test_poses.yaml` (static poses) and `config/test_sequences.yaml` (dynamic motion sequences) as defined in the Motion Sequence Definitions section.

**Requires:** None
**Produces:** `config/test_poses.yaml`, `config/test_sequences.yaml`
**Done when:** YAML files parse correctly and cover at least 4 static poses + 4 dynamic motions

### Task 4.5: Implement Visualizer

Implement `src/evaluation/visualizer.py` per S3. Draw Gap B overlay and test guidance instructions on camera frames.

**Requires:** Task 4.3
**Produces:** `src/evaluation/visualizer.py`
**Done when:** Overlay renders correctly on a test image

### Task 4.6: Implement Replay

Implement `src/evaluation/replay.py` per S4. Load stored sessions, re-process with different parameters, and compare results.

**Requires:** Tasks 4.2, 4.3
**Produces:** `src/evaluation/replay.py`
**Done when:** Unit test shows that changing smoothing_factor produces different gap values on replayed data

### Task 4.7: Update `src/main.py` with `--record`, `--eval`, `--run-test`

Integrate Recorder, Evaluator, and Visualizer into the main control loop per S5. Implement TestRunner for guided test sequences.

**Requires:** Tasks 4.2, 4.3, 4.4, 4.5
**Produces:** Updated `src/main.py`
**Done when:** `python src/main.py --sim --record --eval` runs without errors, records data, and shows gap overlay

### Task 4.8: Create Replay CLI tool

Create the replay command-line interface per S6.

**Requires:** Task 4.6
**Produces:** `src/evaluation/replay.py` (with `__main__` block)
**Done when:** `python -m src.evaluation.replay --session <path> --config <path>` outputs comparison report

### Task 4.9: Write evaluation tests

Create `tests/test_evaluation.py` with unit tests for all evaluation components.

**Requires:** Tasks 4.2, 4.3, 4.6
**Produces:** `tests/test_evaluation.py`
**Done when:** `pytest tests/test_evaluation.py -v` passes all tests

**Test cases:**

- `test_recorder_creates_jsonl`: Record N frames, verify JSONL file exists and each line is valid JSON with correct fields
- `test_recorder_metadata`: Verify metadata.json is written with session summary
- `test_gap_b_zero_error`: When cmd == actual, gap should be zero
- `test_gap_b_known_error`: When cmd and actual differ by known amount, verify correct gap
- `test_static_pose_within_tolerance`: Known pose within tolerance → high accuracy
- `test_static_pose_outside_tolerance`: Known pose outside tolerance → low accuracy
- `test_motion_sequence_correct`: Simulated correct jab trajectory → high score
- `test_motion_sequence_wrong_timing`: Correct trajectory but wrong timing → timing fail
- `test_trajectory_constraint_decrease_increase`: Verify pattern detection
- `test_trajectory_constraint_stable`: Verify stability detection
- `test_replay_different_smoothing`: Replay with different smoothing → different results
- `test_replay_comparison`: Compare original vs replayed → correct improvement metrics

### Task 4.10: Integration test — full eval pipeline

Run `python src/main.py --sim --record --eval --run-test all` and verify:
- Test instructions appear on screen
- Gap overlay updates in real-time
- Session data saved to JSONL after completion
- Benchmark report generated

**Requires:** Tasks 4.7, 4.9
**Done when:** Full test sequence completes, report generated with Gap A + Gap B scores

---

## Agent-Driven Improvement Workflow

Once the eval pipeline is in place, the improvement loop works as follows:

```
1. USER: runs --record --eval session, collects data
2. USER: asks agent "analyze this session and improve accuracy"
3. AGENT:
   a. Loads session JSONL + metadata, reads summary stats
   b. Identifies worst joint and worst motion
   c. Analyzes error patterns (overshoot, jitter, bias, etc.)
   d. Proposes parameter or code change
   e. Runs replay to validate improvement offline
   f. Applies change and commits
4. USER: runs another --record --eval session to verify live
5. REPEAT until target accuracy reached
```

### Tunable Parameters (what the agent can adjust)

| Parameter | File | Effect |
|-----------|------|--------|
| `smoothing_factor` | `config/settings.yaml` | Higher = smoother but slower |
| `dead_zone_deg` | `config/settings.yaml` | Higher = stable but less responsive |
| `joint_scale[i]` | `src/motion_mapper.py` | Per-joint movement multiplier |
| `joint_offset[i]` | `src/motion_mapper.py` | Per-joint zero-point correction |
| `pybullet_kp` | `src/simulated_robot.py` | Position control proportional gain |
| `pybullet_kd` | `src/simulated_robot.py` | Position control derivative gain |
| `z_smoothing` | `src/angle_calculator.py` | Depth noise filter strength |
| `visibility_threshold` | `config/settings.yaml` | Min confidence to use a landmark |
| `elbow_dead_zone` | `src/angle_calculator.py` | Elbow angle noise suppression |

---

## Acceptance Criteria

- [ ] `python src/main.py --sim --record` saves session data to `data/sessions/<session_id>/frames.jsonl`
- [ ] Each line in JSONL file is valid JSON with all schema fields
- [ ] Session metadata written to `data/sessions/<session_id>/metadata.json`
- [ ] `python src/main.py --sim --eval` shows real-time Gap B overlay on camera feed
- [ ] Gap B overlay shows per-joint error bars with color coding (green/yellow/red)
- [ ] `python src/main.py --sim --run-test static` guides through 4+ static poses and reports Gap A
- [ ] `python src/main.py --sim --run-test dynamic` guides through 4+ motion sequences and evaluates trajectory + timing
- [ ] `python -m src.evaluation.replay --session <path> --config <path>` outputs before/after comparison
- [ ] Replay shows measurable difference when smoothing_factor is changed
- [ ] `pytest tests/test_evaluation.py` passes all tests
- [ ] `python src/main.py --sim` still works without `--record` or `--eval` (no regression)
- [ ] Recording a 2-minute session produces reasonable file sizes (JSONL < 100MB)

---

## Future: Storage Format Migration

JSONL is the right choice for the first iteration — zero dependencies, human-readable, easy to debug. Revisit storage format when any of these triggers are hit:

### When to migrate

| Trigger | Symptom | Action |
|---------|---------|--------|
| Sessions > 10 minutes regularly | JSONL files > 200MB, slow to load | Consider Parquet or SQLite |
| Cross-session analysis needed | Loading 50+ sessions for comparison is slow | Consider SQLite for queryability |
| Columnar access patterns | "Give me only elbow angles across all sessions" | Parquet excels at this |
| Disk space pressure | Many sessions accumulating | Parquet offers ~10x compression vs JSON |

### Migration candidates

1. **Parquet** (via `pyarrow`) — Best for columnar analytics, compact storage, fast reads. Natural upgrade path since the schema maps directly to a flat table. Requires one new dependency.
2. **SQLite** — Best if ad-hoc queries become common ("find all sessions where gap_b > 10°"). Built into Python stdlib. Good for metadata queries + frame storage.
3. **DuckDB** — Combines SQLite's queryability with Parquet's columnar performance. One dependency. Could query JSONL files directly as a stepping stone.

### Migration path

The `Recorder` and `Replay` classes encapsulate all storage I/O. Migration only touches:
- `Recorder.record_frame()` — write path
- `Recorder.end_session()` — finalization
- `Replay.load_session()` — read path

The rest of the pipeline (Evaluator, Visualizer, main.py integration) operates on in-memory dicts/arrays and is format-agnostic. This means migration is a localized change with no API surface impact.
